{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHILOGjOQbsQ"
   },
   "source": [
    "&emsp;&emsp;**pytorch提供了一种特殊的数据类型，张量Tensor。可以将标量视为0维张量，向量视为1维张量，矩阵视为2维张量。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "函数Tensor创建一维张量为： tensor([1., 2., 3., 4.])\n",
      "函数Tensor创建二维张量为： tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "函数linspace创建一维张量为(包含最后一个元素)： tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
      "        1.0000])\n",
      "函数zeros创建一维张量为： tensor([0., 0., 0.])\n",
      "函数zeros创建二维张量为： tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "函数ones创建一维张量为： tensor([1., 1., 1.])\n",
      "函数ones创建二维张量为： tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "函数eye创建二维张量为： tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "函数rand创建一维张量为： tensor([0.0683, 0.1723, 0.2247])\n",
      "函数rand创建二维张量为： tensor([[0.1308, 0.2226, 0.9229],\n",
      "        [0.7646, 0.7409, 0.8067],\n",
      "        [0.0128, 0.9964, 0.5150]])\n",
      "函数randn创建一维张量为： tensor([0.1876, 0.5057, 0.3861])\n",
      "函数randn创建二维张量为： tensor([[-0.0674,  0.7765, -0.3260],\n",
      "        [-0.8839,  0.3914,  0.1709],\n",
      "        [-1.0421, -0.6725, -0.8235]])\n",
      "转换张量类型： tensor([ 1.3536, -0.3279, -0.9971], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print('函数Tensor创建一维张量为：', torch.Tensor([1, 2, 3, 4]))\n",
    "print('函数Tensor创建二维张量为：', torch.Tensor([[1, 2], [3, 4]]))\n",
    "\n",
    "print('函数linspace创建一维张量为(包含最后一个元素)：', torch.linspace(0, 1, 10))\n",
    "\n",
    "print('函数zeros创建一维张量为：', torch.zeros(3))\n",
    "print('函数zeros创建二维张量为：', torch.zeros((3, 1)))\n",
    "\n",
    "print('函数ones创建一维张量为：', torch.ones(3))\n",
    "print('函数ones创建二维张量为：', torch.ones((3, 1)))\n",
    "\n",
    "print('函数eye创建二维张量为：', torch.eye(3))\n",
    "\n",
    "print('函数rand创建一维张量为：', torch.rand(3))\n",
    "print('函数rand创建二维张量为：', torch.rand(3, 3))\n",
    "\n",
    "print('函数randn创建一维张量为：', torch.randn(3))\n",
    "print('函数randn创建二维张量为：', torch.randn(3, 3))\n",
    "\n",
    "# 默认的是CPU张量且是32位浮点型\n",
    "print('转换张量类型：', torch.randn(3, dtype=torch.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尺寸，维数，元素类型, 转置： torch.Size([2, 3]) 2 torch.float32 tensor([[1., 4.],\n",
      "        [2., 5.],\n",
      "        [3., 6.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print('尺寸，维数，元素类型, 转置：', x.shape, x.ndim, x.dtype, x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "改变张量形状(创建副本)： tensor([1., 2., 3., 4., 5., 6.])\n",
      "增加张量维度(创建副本)： torch.Size([1, 2, 3]) torch.Size([2, 3, 1])\n",
      "删除张量维度(创建副本)： torch.Size([1, 3, 4])\n",
      "交换张量维度(创建副本)： torch.Size([1, 28, 32, 3])\n",
      "数组与张量之间的相互转换，共享内存，改变其中一个另一个也会变化\n",
      "张量转换为数组(创建副本)： [[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "数组转换为张量(创建副本)： tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print('改变张量形状(创建副本)：', x.view(6))\n",
    "print('增加张量维度(创建副本)：', x.unsqueeze(0).shape, x.unsqueeze(2).shape)\n",
    "x = torch.rand(1,1,3,4)\n",
    "print('删除张量维度(创建副本)：', x.squeeze(0).shape)\n",
    "x = torch.rand(1,3,28,32)\n",
    "print('交换张量维度(创建副本)：', x.permute(0,2,3,1).shape)   # 比如将CNN的输入由nchw变为nhwc\n",
    "\n",
    "print('数组与张量之间的相互转换，共享内存，改变其中一个另一个也会变化')\n",
    "x = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print('张量转换为数组(创建副本)：', x.numpy())\n",
    "import numpy as np\n",
    "a = np.array([1,2,3])\n",
    "print('数组转换为张量(创建副本)：', torch.from_numpy(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 索引切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "二维张量的索引 tensor(1.)\n",
      "二维张量的冒号表达式切片 tensor([1., 2.])\n",
      "二维张量的列表切片 tensor([1., 2.])\n",
      "二维张量的逻辑型切片 tensor([1., 2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "print('二维张量的索引', x[0][0])\n",
    "print('二维张量的冒号表达式切片', x[0, :])\n",
    "print('二维张量的列表切片', x[0, [0, 1]])\n",
    "print('二维张量的逻辑型切片', x[0, [True, True]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor的函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "默认求最大值, 轴为0对列求最大值，轴为1对行求最大值 tensor(4.) torch.return_types.max(\n",
      "values=tensor([3., 4.]),\n",
      "indices=tensor([1, 1])) torch.return_types.max(\n",
      "values=tensor([2., 4.]),\n",
      "indices=tensor([1, 1]))\n",
      "默认求最小值, 轴为0对列求最小值，轴为1对行求最小值 tensor(1.) torch.return_types.min(\n",
      "values=tensor([1., 2.]),\n",
      "indices=tensor([0, 0])) torch.return_types.min(\n",
      "values=tensor([1., 3.]),\n",
      "indices=tensor([0, 0]))\n",
      "默认求最大值索引, 轴为0对列求最大值索引，轴为1对行求最大值索引 tensor(3) tensor([1, 1]) tensor([1, 1])\n",
      "默认求最小值索引, 轴为0对列求最小值索引，轴为1对行求最小值索引 tensor(0) tensor([0, 0]) tensor([0, 0])\n",
      "默认求和, 轴为0对列求和，轴为1对行求和 tensor(10.) tensor([4., 6.]) tensor([3., 7.])\n",
      "默认求积, 轴为0对列求积，轴为1对行求积 tensor(24.) tensor([3., 8.]) tensor([ 2., 12.])\n",
      "轴为0对列求累加和，轴为1对行求累加和 tensor([[1., 2.],\n",
      "        [4., 6.]]) tensor([[1., 3.],\n",
      "        [3., 7.]])\n",
      "轴为0对列求累加积，轴为1对行求累加积 tensor([[1., 2.],\n",
      "        [3., 8.]]) tensor([[ 1.,  2.],\n",
      "        [ 3., 12.]])\n",
      "默认求均值, 轴为0对列求均值，轴为1对行求均值 tensor(2.5000) tensor([2., 3.]) tensor([1.5000, 3.5000])\n",
      "**默认求中位数, 轴为0对列求中位数，轴为1对行求中位数**(和理解的不一样) tensor(2.) torch.return_types.median(\n",
      "values=tensor([1., 2.]),\n",
      "indices=tensor([0, 0])) torch.return_types.median(\n",
      "values=tensor([1., 3.]),\n",
      "indices=tensor([0, 0]))\n",
      "默认求分位数, 轴为0对列求分位数，轴为1对行求分位数 tensor(1.7500) tensor([1.5000, 2.5000]) tensor([1.2500, 3.2500])\n",
      "默认求方差, 轴为0对列求方差，轴为1对行求方差 tensor(1.6667) tensor([2., 2.]) tensor([0.5000, 0.5000])\n",
      "默认求标准差, 轴为0对列求标准差，轴为1对行求标准差 tensor(1.2910) tensor([1.4142, 1.4142]) tensor([0.7071, 0.7071])\n",
      "默认对行求协方差： tensor([[0.5000, 0.5000],\n",
      "        [0.5000, 0.5000]])\n",
      "默认对行求相关系数： tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "向量的点乘或者内积或者数量积： tensor(6.)\n",
      "向量的叉乘或者外积或者向量积： tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "print('默认求最大值, 轴为0对列求最大值，轴为1对行求最大值', torch.max(x), torch.max(x, axis=0), torch.max(x, axis=1))\n",
    "\n",
    "print('默认求最小值, 轴为0对列求最小值，轴为1对行求最小值', torch.min(x), torch.min(x, axis=0), torch.min(x, axis=1))\n",
    "\n",
    "print('默认求最大值索引, 轴为0对列求最大值索引，轴为1对行求最大值索引', torch.argmax(x), torch.argmax(x, axis=0), torch.argmax(x, axis=1))\n",
    "\n",
    "print('默认求最小值索引, 轴为0对列求最小值索引，轴为1对行求最小值索引', torch.argmin(x), torch.argmin(x, axis=0), torch.argmin(x, axis=1))\n",
    "\n",
    "print('默认求和, 轴为0对列求和，轴为1对行求和', torch.sum(x), torch.sum(x,dim=0), torch.sum(x,dim=1))\n",
    "\n",
    "print('默认求积, 轴为0对列求积，轴为1对行求积', torch.prod(x), torch.prod(x,dim=0), torch.prod(x,dim=1))\n",
    "\n",
    "print('轴为0对列求累加和，轴为1对行求累加和', torch.cumsum(x, dim=0), torch.cumsum(x, dim=1))\n",
    "\n",
    "print('轴为0对列求累加积，轴为1对行求累加积', torch.cumprod(x, dim=0), torch.cumprod(x, dim=1))\n",
    "\n",
    "print('默认求均值, 轴为0对列求均值，轴为1对行求均值', torch.mean(x), torch.mean(x, dim=0), torch.mean(x, dim=1))\n",
    "\n",
    "print('**默认求中位数, 轴为0对列求中位数，轴为1对行求中位数**(和理解的不一样)', torch.median(x), torch.median(x, dim=0), torch.median(x, dim=1))\n",
    "\n",
    "print('默认求分位数, 轴为0对列求分位数，轴为1对行求分位数', torch.quantile(x, 0.25), torch.quantile(x, 0.25, dim=0), torch.quantile(x, 0.25, dim=1))\n",
    "\n",
    "print('默认求方差, 轴为0对列求方差，轴为1对行求方差', torch.var(x), torch.var(x, dim=0), torch.var(x, dim=1))\n",
    "\n",
    "print('默认求标准差, 轴为0对列求标准差，轴为1对行求标准差', torch.std(x), torch.std(x, dim=0), torch.std(x, dim=1))\n",
    "\n",
    "print('默认对行求协方差：', torch.cov(x))\n",
    "\n",
    "print('默认对行求相关系数：', torch.corrcoef(x))\n",
    "\n",
    "x = torch.Tensor([1,2,3])\n",
    "y = torch.Tensor([1,1,1])\n",
    "print('向量的点乘或者内积或者数量积：', torch.dot(x,y))\n",
    "print('向量的叉乘或者外积或者向量积：', torch.multiply(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其他函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机种子： <torch._C.Generator object at 0x0000026A07DD04B0>\n",
      "张量的纵向拼接(不增加维度)： tensor([[0.7576, 0.2793],\n",
      "        [0.4031, 0.7347],\n",
      "        [0.0293, 0.7999]])\n",
      "张量的横向拼接(不增加维度)： tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293, 0.7999]])\n",
      "tensor([[0.3971, 0.7544, 0.5695, 0.4388],\n",
      "        [0.6387, 0.5247, 0.6826, 0.3051],\n",
      "        [0.4635, 0.4550, 0.5725, 0.4980]])\n",
      "张量的纵向分割： (tensor([[0.3971, 0.7544, 0.5695, 0.4388],\n",
      "        [0.6387, 0.5247, 0.6826, 0.3051]]), tensor([[0.4635, 0.4550, 0.5725, 0.4980]]))\n",
      "张量的横向分割： (tensor([[0.3971, 0.7544],\n",
      "        [0.6387, 0.5247],\n",
      "        [0.4635, 0.4550]]), tensor([[0.5695, 0.4388],\n",
      "        [0.6826, 0.3051],\n",
      "        [0.5725, 0.4980]]))\n",
      "张量的纵向堆叠(增加维度)： tensor([[[0.9371, 0.6556]],\n",
      "\n",
      "        [[0.3138, 0.1980]],\n",
      "\n",
      "        [[0.4162, 0.2843]]]) torch.Size([3, 1, 2])\n",
      "张量的横向堆叠(增加维度)： tensor([[[0.9371, 0.6556],\n",
      "         [0.3138, 0.1980],\n",
      "         [0.4162, 0.2843]]]) torch.Size([1, 3, 2])\n",
      "tensor([[0.3398, 0.5239, 0.7981, 0.7718],\n",
      "        [0.0112, 0.8100, 0.6397, 0.9743],\n",
      "        [0.8300, 0.0444, 0.0246, 0.2588]])\n",
      "张量的纵向分解(和纵向分割同)： (tensor([[0.3398, 0.5239, 0.7981, 0.7718],\n",
      "        [0.0112, 0.8100, 0.6397, 0.9743]]), tensor([[0.8300, 0.0444, 0.0246, 0.2588]]))\n",
      "张量的横向分解(和横向分割同)： (tensor([[0.3398, 0.5239],\n",
      "        [0.0112, 0.8100],\n",
      "        [0.8300, 0.0444]]), tensor([[0.7981, 0.7718],\n",
      "        [0.6397, 0.9743],\n",
      "        [0.0246, 0.2588]]))\n"
     ]
    }
   ],
   "source": [
    "print('随机种子：', torch.manual_seed(1))\n",
    "x = torch.rand(1, 2)\n",
    "y = torch.rand(1, 2)\n",
    "z = torch.rand(1, 2)\n",
    "print('张量的纵向拼接(不增加维度)：', torch.cat([x,y,z],dim=0))\n",
    "print('张量的横向拼接(不增加维度)：', torch.cat([x,y,z],dim=1))\n",
    "x = torch.rand(3, 4)\n",
    "print(x)\n",
    "print('张量的纵向分割：', torch.split(x, 2, dim=0))\n",
    "print('张量的横向分割：', torch.split(x, 2, dim=1))\n",
    "x = torch.rand(1, 2)\n",
    "y = torch.rand(1, 2)\n",
    "z = torch.rand(1, 2)\n",
    "print('张量的纵向堆叠(增加维度)：', torch.stack([x,y,z],dim=0), torch.stack([x,y,z],dim=0).shape)\n",
    "print('张量的横向堆叠(增加维度)：', torch.stack([x,y,z],dim=1), torch.stack([x,y,z],dim=1).shape)\n",
    "x = torch.rand(3, 4)\n",
    "print(x)\n",
    "print('张量的纵向分解(和纵向分割同)：', torch.chunk(x, 2, dim=0))\n",
    "print('张量的横向分解(和横向分割同)：', torch.chunk(x, 2, dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuda张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 2)\n",
    "# 方法一，使用cuda方法\n",
    "# print(x.cuda().device)\n",
    "# 方法二，存在多个gpu时使用to方法确定哪个\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    x=x.to(device)\n",
    "    print(x.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor进阶"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset和DataLoader是加载数据集的两个重要工具类，Dataset用于构造支持索引的数据集，DataLoader用来在Dataset里面取出一组数据（Mini-Batch）供训练时快速使用。\n",
    "\n",
    "Dataset是抽象类不能实例化，所以在使用Dataset的时候需要定义自己的数据集类，也就是Dataset的子类，来继承Dataset类的属性和方法。\n",
    "\n",
    "Dataset可以作为DataLoader的参数，实现基于张量的数据预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset抽象类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模板"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "# 继承Dataset的框架如下，需要重写__getitem__函数和__len__函数，代表数据的索引到真正数据样本的映射，也就是说，使用\n",
    "# 这种方式读取的数据并非直接把所有数据读取出来，而是读取数据的索引或者键值\n",
    "class MyDataset(Dataset):\n",
    "    # 读取数据并预处理,一般是将数据集划分为训练集测试集和验证集\n",
    "    def __init__(self, file):\n",
    "        self.data = ...\n",
    "        \n",
    "    # 实现通过给定的索引遍历数据样本，索引一般范围为[1,batch_num]\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    # 实现返回数据的条数，猜测是为了后面根据batch_size计算batch_num\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7996 7996\n",
      "1996 1996\n"
     ]
    }
   ],
   "source": [
    "# 在创建的dataset类中可根据自己的需求对数据进行处理，以时间序列使用为示例，输入3个时间步，输出1个时间步，batch_size=5\n",
    "class GetTrainTestData(Dataset):\n",
    "    def __init__(self, input_len, output_len, train_rate, is_train=True):\n",
    "        super().__init__()\n",
    "        # 使用sin函数返回10000个时间序列,如果不自己构造数据，就使用numpy,pandas等读取自己的数据为x即可。\n",
    "        # 以下数据组织这块既可以放在init方法里，也可以放在getitem方法里\n",
    "        self.x = torch.sin(torch.arange(0, 1000, 0.1))\n",
    "        self.sample_num = len(self.x)\n",
    "        self.input_len = input_len\n",
    "        self.output_len = output_len\n",
    "        self.train_rate = train_rate\n",
    "        self.src, self.trg = [], []\n",
    "        if is_train:\n",
    "            for i in range(int(self.sample_num*train_rate)-self.input_len-self.output_len):\n",
    "                self.src.append(self.x[i:(i+input_len)])\n",
    "                self.trg.append(self.x[(i+input_len):(i+input_len+output_len)])\n",
    "        else:\n",
    "            for i in range(int(self.sample_num*train_rate), self.sample_num-self.input_len-self.output_len):\n",
    "                self.src.append(self.x[i:(i+input_len)])\n",
    "                self.trg.append(self.x[(i+input_len):(i+input_len+output_len)])\n",
    "        print(len(self.src), len(self.trg))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.src[index], self.trg[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)  # 或者return len(self.trg), src和trg长度一样\n",
    "\n",
    "# 实例化我们定义好的Dataset子类GetTrainTestData\n",
    "data_train = GetTrainTestData(input_len=3, output_len=1, train_rate=0.8, is_train=True)\n",
    "data_test = GetTrainTestData(input_len=3, output_len=1, train_rate=0.8, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "官方给出的DataLoader(\n",
    "\n",
    "        dataset, 通过datasets加载进来的数据集\n",
    "        \n",
    "        batch_size=1,\n",
    "        \n",
    "        shuffle=False,\n",
    "        \n",
    "        sampler=None, 定义从数据集中提取样本的策略，若指定，shuffle必须为False\n",
    "        \n",
    "        batch_sampler=None, 批量采样，每次返回一个Batch大小的索引，和shuffle,sampler参数互斥\n",
    "        \n",
    "        num_workers=0, 用多少子进程加载数据，0表示将数据加载进主进程\n",
    "        \n",
    "        collate_fn=None, 将一小段数据合并成数据列表以形成一个Batch\n",
    "        \n",
    "        pin_memory=False, 是否在将张量返回之前将其复制到Cuda固定的内存中\n",
    "                \n",
    "        drop_last=False, 设置了batch_size的数目后，最后一批数据未必是设置的数据，这时需要丢弃这些数据\n",
    "        \n",
    "        timeout=0, 设置数据的读取时间，超过这个时间还没读取到数据就会报错\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原理就是会默认调用5次__getitem__()生成一个Batch，然后再调用5次生成另一个Batch\n",
    "data_loader_train = DataLoader(data_train, batch_size=5, shuffle=False)\n",
    "data_loader_test = DataLoader(data_test, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性层\n",
    "\n",
    "线性层的输入和输出均是二维张量，形状为[batch_size,size]\n",
    "\n",
    "torch.nn.Linear(\n",
    "\n",
    "    in_features: int, 输入的特征个数\n",
    "    \n",
    "    out_features: int, 输出的特征个数\n",
    "    \n",
    "    bias: bool = True, 代表是否含截距项，默认是有的\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3228],\n",
       "        [-0.1179],\n",
       "        [-0.0733],\n",
       "        [-0.0858],\n",
       "        [ 0.0279],\n",
       "        [ 0.2489],\n",
       "        [ 0.0688],\n",
       "        [ 0.1057],\n",
       "        [ 0.1897],\n",
       "        [ 0.2435]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = torch.nn.Linear(in_features=64,out_features=1)\n",
    "in_put = torch.rand(10, 64)\n",
    "out_put = linear(in_put)\n",
    "out_put"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积层\n",
    "\n",
    "一维卷积层的输入和输出均是三维张量，形状为[batch_size, channels,size]，也就是俗称的ncw\n",
    "\n",
    "torch.nn.Conv1d(\n",
    "\n",
    "    in_channels: int, 输入数据的通道数\n",
    "    \n",
    "    out_channels: int, 卷积产生的通道数，有多少个out_channels就有多少个1维卷积核\n",
    "    \n",
    "    kernel_size: Union[int, Tuple[int]], 卷积核的大小\n",
    "    \n",
    "    stride: Union[int, Tuple[int]] = 1, 卷积的步长，默认为1\n",
    "    \n",
    "    padding: Union[str, int, Tuple[int]] = 0, 为输入的每一条边补充0的层数，默认为0\n",
    "    \n",
    "    dilation: Union[int, Tuple[int]] = 1, 内核元素之间的间距，默认为1\n",
    "    \n",
    "    groups: int = 1, 从输入通道到输出通道的阻塞连接数，默认为1\n",
    "    \n",
    "    bias: bool = True, 是否含偏置项，默认有\n",
    "    \n",
    "    padding_mode: str = 'zeros',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1617,  0.1806,  0.2450],\n",
       "         [-0.0380,  0.0387,  0.0828],\n",
       "         [-0.1626, -0.1277, -0.1465],\n",
       "         [-0.2871, -0.0376, -0.1531],\n",
       "         [-0.5297, -0.5352, -0.4669],\n",
       "         [-0.4317, -0.2920, -0.4088],\n",
       "         [-0.3357, -0.2621, -0.1426],\n",
       "         [ 0.1885,  0.3660,  0.2016],\n",
       "         [ 0.2500,  0.0433,  0.1918],\n",
       "         [ 0.5642,  0.6080,  0.6452]],\n",
       "\n",
       "        [[-0.0969,  0.1835,  0.0808],\n",
       "         [ 0.0488, -0.0736,  0.1645],\n",
       "         [-0.3448, -0.3783, -0.1332],\n",
       "         [-0.2401, -0.0740, -0.6692],\n",
       "         [-0.4489, -0.6314, -0.5156],\n",
       "         [-0.3117, -0.5359, -0.1686],\n",
       "         [-0.3189, -0.1653, -0.5110],\n",
       "         [ 0.4863,  0.1953,  0.2709],\n",
       "         [-0.1802,  0.1695,  0.3260],\n",
       "         [ 0.6678,  0.5125,  0.8161]],\n",
       "\n",
       "        [[ 0.1690,  0.1958, -0.0712],\n",
       "         [ 0.1705, -0.1422,  0.3010],\n",
       "         [-0.1789, -0.0673,  0.0564],\n",
       "         [-0.3014, -0.4840, -0.2234],\n",
       "         [-0.4514, -0.6428, -0.1777],\n",
       "         [-0.4841, -0.3274, -0.4911],\n",
       "         [-0.3333, -0.5107, -0.4865],\n",
       "         [ 0.4397, -0.1646,  0.4234],\n",
       "         [ 0.1418,  0.1721,  0.2530],\n",
       "         [ 0.7815,  0.6592,  0.7882]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1 = torch.nn.Conv1d(in_channels=256, out_channels=10, kernel_size=2, stride=1, padding=0)\n",
    "in_put = torch.rand(3, 256, 4)\n",
    "out_put = conv1(in_put)\n",
    "out_put\n",
    "# out_size的计算公式为 (input_size+2*padding-dilation*(kernel_size-1)-1)/stride+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二维卷积层的输入和输出均是四维张量，形状为[batch_size, channels,height,width]，也就是俗称的nchw\n",
    "\n",
    "torch.nn.Conv2d(\n",
    "\n",
    "    in_channels: int, 输入数据的通道数\n",
    "    \n",
    "    out_channels: int, 卷积产生的通道数，有多少个out_channels就有多少个2维卷积核\n",
    "    \n",
    "    kernel_size: Union[int, Tuple[int]], 卷积核的大小\n",
    "    \n",
    "    stride: Union[int, Tuple[int]] = 1, 卷积的步长，默认为1\n",
    "    \n",
    "    padding: Union[str, int, Tuple[int]] = 0, 为输入的每一条边补充0的层数，默认为0\n",
    "    \n",
    "    dilation: Union[int, Tuple[int]] = 1, 内核元素之间的间距，默认为1\n",
    "    \n",
    "    groups: int = 1, 从输入通道到输出通道的阻塞连接数，默认为1\n",
    "    \n",
    "    bias: bool = True, 是否含偏置项，默认有\n",
    "    \n",
    "    padding_mode: str = 'zeros',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.4259e-01, -4.2189e-01],\n",
       "          [-6.1243e-01, -3.4766e-01],\n",
       "          [-4.9598e-01, -6.6917e-01],\n",
       "          [-5.4419e-01, -4.8589e-01]],\n",
       "\n",
       "         [[ 8.9927e-02,  1.8231e-01],\n",
       "          [ 1.3362e-02, -2.8967e-01],\n",
       "          [-9.3568e-02,  1.0606e-01],\n",
       "          [ 9.5853e-02, -1.8042e-01]],\n",
       "\n",
       "         [[-7.0261e-02,  6.3861e-01],\n",
       "          [-2.5721e-02,  5.2383e-02],\n",
       "          [ 1.7181e-01, -1.5789e-01],\n",
       "          [-1.3158e-01, -1.3138e-01]],\n",
       "\n",
       "         [[-1.3120e-01,  2.6718e-01],\n",
       "          [-1.1516e-01, -1.9011e-01],\n",
       "          [ 5.2660e-02, -9.9572e-02],\n",
       "          [ 2.3861e-02,  3.0720e-02]]],\n",
       "\n",
       "\n",
       "        [[[-7.5065e-01, -2.7599e-01],\n",
       "          [-3.5668e-01, -4.2564e-01],\n",
       "          [-3.9874e-01, -2.7068e-01],\n",
       "          [-3.0599e-01, -6.7458e-01]],\n",
       "\n",
       "         [[ 1.7572e-01, -1.5247e-01],\n",
       "          [-3.2685e-01, -2.2420e-02],\n",
       "          [-2.7031e-02, -8.5166e-02],\n",
       "          [-4.8411e-02,  4.0834e-01]],\n",
       "\n",
       "         [[-3.2239e-02,  7.4931e-02],\n",
       "          [-6.9917e-02,  1.6960e-01],\n",
       "          [-1.8021e-01, -3.5162e-02],\n",
       "          [ 2.1989e-01,  1.1706e-01]],\n",
       "\n",
       "         [[-6.3965e-04, -5.6235e-02],\n",
       "          [-8.7815e-02,  1.5213e-01],\n",
       "          [-1.1712e-02,  2.9864e-02],\n",
       "          [ 7.9720e-02,  1.2470e-01]]],\n",
       "\n",
       "\n",
       "        [[[-1.9313e-01, -3.6114e-01],\n",
       "          [-6.2412e-01, -4.0696e-01],\n",
       "          [-5.9647e-01, -4.7570e-01],\n",
       "          [-2.8544e-01, -4.7112e-01]],\n",
       "\n",
       "         [[-2.6528e-01,  1.7778e-02],\n",
       "          [ 1.6691e-01,  3.4609e-02],\n",
       "          [ 8.5439e-02, -1.2438e-02],\n",
       "          [-8.3678e-02,  9.8738e-02]],\n",
       "\n",
       "         [[-8.9172e-02,  1.0029e-01],\n",
       "          [-1.2926e-01, -1.2353e-01],\n",
       "          [-1.2164e-01, -7.3975e-02],\n",
       "          [-7.3408e-03,  6.0635e-02]],\n",
       "\n",
       "         [[-5.2000e-02,  1.5106e-01],\n",
       "          [ 4.6537e-02, -7.1372e-02],\n",
       "          [ 1.7973e-02, -1.2581e-02],\n",
       "          [-1.8955e-02,  4.1440e-02]]]], grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2 = torch.nn.Conv2d(1, 4, (2, 3))\n",
    "in_put = torch.rand(3, 1, 5, 4)\n",
    "out_put = conv2(in_put)\n",
    "out_put\n",
    "# out_put_h的计算公式为（in_put_h+2*padding[0]-dilation[0]*(kernei_size[0]-1)-1)/stride[0]+1\n",
    "# out_put_w的计算公式为（in_put_w+2*padding[1]-dilation[1]*(kernei_size[1]-1)-1)/stride[1]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量规范层\n",
    "\n",
    "在卷积层之后添加BatchNorm1d进行数据归一化处理，这使得数据在进行ReLU之前不会因为数据过大而导致网络性能的不稳定。\n",
    "\n",
    "一维批量规范层的输入和输出均是四维张量，形状为[batch_size, channels,width]，也就是俗称的ncw\n",
    "\n",
    "torch.nn.BatchNorm1d(\n",
    "\n",
    "    num_features: int, 输入数据的通道数/输入张量的维度,即ncw中的c\n",
    "    \n",
    "    eps: float, 为数值稳定性添加到分母的值，默认为1e-5\n",
    "    \n",
    "    momentum: float，为动态均值和动态方差所使用的动量，默认为0.1\n",
    "    \n",
    "    assine: bool, 设置为True时，给该层添加可学习的放射变换参数，默认为True\n",
    "    \n",
    "    track_running_stats: bool，设置为True时，记录训练过程中的均值和方差，默认为True\n",
    "    \n",
    ")\n",
    "\n",
    "二维批量规范层的输入和输出均是四维张量，形状为[batch_size, channels,height,width]，也就是俗称的nchw\n",
    "\n",
    "torch.nn.BatchNorm2d(\n",
    "\n",
    "    num_features: int, 输入数据的通道数/输入张量的维度,即nchw中的c\n",
    "    \n",
    "    eps: float, 为数值稳定性添加到分母的值，默认为1e-5\n",
    "    \n",
    "    momentum: float，为动态均值和动态方差所使用的动量，默认为0.1\n",
    "    \n",
    "    assine: bool, 设置为True时，给该层添加可学习的放射变换参数，默认为True\n",
    "    \n",
    "    track_running_stats: bool，设置为True时，记录训练过程中的均值和方差，默认为True\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7271],\n",
      "         [-1.4736]],\n",
      "\n",
      "        [[ 0.6272],\n",
      "         [ 1.7446]]]) tensor([[[-1.0000],\n",
      "         [-1.0000]],\n",
      "\n",
      "        [[ 1.0000],\n",
      "         [ 1.0000]]], grad_fn=<NativeBatchNormBackward0>)\n",
      "tensor([[[[-2.1242,  0.0811],\n",
      "          [ 0.6823, -0.0938]],\n",
      "\n",
      "         [[-0.1948, -0.5301],\n",
      "          [ 0.2689,  0.0095]]]]) tensor([[[[-1.6665,  0.4210],\n",
      "          [ 0.9901,  0.2554]],\n",
      "\n",
      "         [[-0.2846, -1.4321],\n",
      "          [ 1.3023,  0.4144]]]], grad_fn=<NativeBatchNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Bat = torch.nn.BatchNorm1d(2)\n",
    "input = torch.randn(2, 2, 1)\n",
    "output = Bat(input)\n",
    "print(input, output)\n",
    "\n",
    "Bat = torch.nn.BatchNorm2d(2)\n",
    "input = torch.randn(1, 2, 2, 2)\n",
    "output = Bat(input)\n",
    "print(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN层\n",
    "\n",
    "输入三维张量[时间步数，批量大小，特征维度]，输出[时间步数，批量大小，隐藏神经元个数]，隐藏层在最后时间步的隐藏状态，\n",
    "\n",
    "当隐藏层有多层时，每一层的隐藏状态都会记录在该变量中[层数，批量大小，隐藏单元个数]\n",
    "\n",
    "\n",
    "torch.nn.RNN(\n",
    "\n",
    "    input_size, 输入特征的维度\n",
    "    \n",
    "    hidden_size, 隐藏层神经元个数，或者也叫输出特征的维度\n",
    "    \n",
    "    num_layers=1, 隐藏层个数\n",
    "    \n",
    "    nonlinearity=tanh, 激活函数\n",
    "    \n",
    "    bias=True, 是否含偏置项\n",
    "    \n",
    "    batch_first=False, 输入数据的形式\n",
    "    \n",
    "    dropout=0, 是否应用dropout\n",
    "    \n",
    "    bidirectional=False，是否使用双向的RNN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 2, 32]) torch.Size([35, 2, 2]) 1 torch.Size([1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "feature_size = 32\n",
    "num_steps = 35\n",
    "batch_size = 2\n",
    "num_hiddens = 2\n",
    "X = torch.rand(num_steps, batch_size, feature_size)\n",
    "RNN_layer = torch.nn.RNN(input_size=feature_size, hidden_size=num_hiddens)\n",
    "Y, state_new = RNN_layer(X)\n",
    "print(X.shape, Y.shape, len(state_new), state_new.shape)\n",
    "# torch.Size([35, 2, 32]) torch.Size([35, 2, 2]) 1 torch.Size([1, 2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM层\n",
    "\n",
    "输入三维张量[时间步数，批量大小，特征维度]，输出[时间步数，批量大小，隐藏神经元个数]，隐藏层在最后时间步的隐藏状态，单元状态\n",
    "\n",
    "当隐藏层有多层时，每一层的隐藏状态和单元状态都会记录在该变量中[层数，批量大小，隐藏单元个数]\n",
    "\n",
    "torch.nn.LSTM(\n",
    "\n",
    "    input_size, 输入特征的维度\n",
    "    \n",
    "    hidden_size, 隐藏层神经元和记忆单元个数，或者也叫输出特征的维度\n",
    "    \n",
    "    num_layers=1, 隐藏层个数\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20])\n",
      "torch.Size([4, 3, 20])\n",
      "torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "# 构建4层的LSTM,输入的每个词用10维向量表示,隐藏单元和记忆单元的尺寸是20\n",
    "lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=4)\n",
    "\n",
    "# 输入的x:其中batch_size是3表示有三句话,seq_len=5表示每句话5个单词,feature_len=10表示每个单词表示为长10的向量\n",
    "x = torch.randn(5, 3, 10)\n",
    "# 前向计算过程,这里不传入h_0和C_0则会默认初始化\n",
    "out, (h, c) = lstm(x)\n",
    "print(out.shape)  # torch.Size([5, 3, 20]) \n",
    "print(h.shape)  # torch.Size([4, 3, 20]) \n",
    "print(c.shape)  # torch.Size([4, 3, 20]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数\n",
    "\n",
    "激活函数的作用是为网络提供非线性的建模能力。\n",
    "\n",
    "·连续可导，只允许在小数点上不可导；\n",
    "\n",
    "·导数尽可能简单保证网络的计算效率；\n",
    "\n",
    "·导函数要在一个合适的区间内，保证训练的效率和稳定性。\n",
    "\n",
    "使用顺序：ReLU->LeakyReLU->Tanh->Sigmoid。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid函数\n",
    "\n",
    "Sigmoid函数很好地解释了神经元在受到刺激的情况下是否被激活和向后传递的情景，当取值接近0时几乎没有被激活，当取值接近1\n",
    "\n",
    "几乎完全被激活，缺点是：\n",
    "\n",
    "·Sigmoid函数容易出现梯度消失，甚至小概率会出现梯度爆炸\n",
    "\n",
    "·解析式含有幂函数，计算机在求解是会比较耗时\n",
    "\n",
    "·Sigmoid输出不是0均值的，这会造成后面层里的神经元的输入是非零均值的信号，从而对梯度产生影响，使得收敛缓慢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3565,  0.4108, -1.8645, -1.9222])\n",
      "tensor([0.7952, 0.6013, 0.1342, 0.1276])\n"
     ]
    }
   ],
   "source": [
    "input_x = torch.randn(4)\n",
    "print(input_x)\n",
    "output = torch.sigmoid(input_x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh函数\n",
    "\n",
    "tanh(x)=2sigmoid(2x)-1:将输出值映射到-1到1，但是也有缺点\n",
    "\n",
    "·也存在梯度消失和梯度爆炸的问题\n",
    "\n",
    "·也含有幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.5347, -0.2651, -0.2508,  0.1727])\n",
      "tensor([-0.4890, -0.2591, -0.2457,  0.1710])\n"
     ]
    }
   ],
   "source": [
    "input_x = torch.randn(4)\n",
    "print(input_x)\n",
    "output = torch.tanh(input_x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU函数\n",
    "\n",
    "·收敛快，计算简单，具有单侧抑制，宽兴奋边界的生物学合理性，可以缓解梯度消失的问题\n",
    "\n",
    "·有时比较脆弱，可能会导致神经元的死亡，比如很大的梯度经过ReLU单元之后，权值的更新结果可能是0，在此之后它将永远\n",
    "\n",
    "能再被激活，即神经元死亡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7852,  0.9937,  0.4717, -1.4807])\n",
      "tensor([0.7852, 0.9937, 0.4717, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "input_x = torch.randn(4)\n",
    "print(input_x)\n",
    "output = torch.nn.functional.relu(input_x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LeakyReLU函数\n",
    "Leaky Relu解决了一部分Relu存在的可能杀死神经元的问题，负轴具有倾斜的斜率保证负轴信息的存在性，但是在使用中并非总是优于ReLU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4817,  0.4851, -2.0000,  0.4409])\n",
      "tensor([-0.0048,  0.4851, -0.0200,  0.4409])\n"
     ]
    }
   ],
   "source": [
    "input_x = torch.randn(4)\n",
    "print(input_x)\n",
    "output = torch.nn.functional.leaky_relu(input_x, 0.01)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module抽象类\n",
    "\n",
    "需要重写__init__()构造函数与forward()函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (linear1): Linear(in_features=96, out_features=1024, bias=True)\n",
      "  (relu1): ReLU(inplace=True)\n",
      "  (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (linear2): Linear(in_features=1024, out_features=6272, bias=True)\n",
      "  (relu2): ReLU(inplace=True)\n",
      "  (batchnorm1d_2): BatchNorm1d(6272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (ConvTranspose2d): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__() # 使用父类的方法初始化子类\n",
    "        self.linear1 = torch.nn.Linear(96, 1024)  # [96,1024]\n",
    "        self.relu1 = torch.nn.ReLU(True)\n",
    "        self.batchnorm1d_1 = torch.nn.BatchNorm1d(1024)\n",
    "        self.linear2 = torch.nn.Linear(1024, 7 * 7 * 128)  # [1024,6272]\n",
    "        self.relu2 = torch.nn.ReLU(True)\n",
    "        self.batchnorm1d_2 = torch.nn.BatchNorm1d(7 * 7 * 128)\n",
    "        self.ConvTranspose2d = nn.ConvTranspose2d(128, 64, 4, 2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batchnorm1d_1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batchnorm1d_2(x)\n",
    "        x = self.ConvTranspose2d(x)\n",
    "        return x\n",
    "\n",
    "model = MyNet()\n",
    "print(model)\n",
    "# 运行结果为：\n",
    "# MyNet(\n",
    "#   (linear1): Linear(in_features=96, out_features=1024, bias=True)\n",
    "#   (relu1): ReLU(inplace=True)\n",
    "#   (batchnorm1d_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (linear2): Linear(in_features=1024, out_features=6272, bias=True)\n",
    "#   (relu2): ReLU(inplace=True)\n",
    "#   (batchnorm1d_2): BatchNorm1d(6272, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "#   (ConvTranspose2d): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params(iterable)为待优化参数的iterable或者是定义了参数组的dict\n",
    "# lr(float)为学习率\n",
    "# momentum(float,可选)为动量因子(默认：0)，选择合适的参数可以实现动量优化算法\n",
    "# dampening(float,可选)为动量的抑制因子(默认：0)\n",
    "# weight_decay(float,可选)为权值衰减，选择一个合适的权值衰减系数非常重要(默认：0)\n",
    "# nesterov(bool,可选)使用Nesterov动量\n",
    "torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, dampening=0, weight_decay=0, nesterov=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adagrad (\n",
       "Parameter Group 0\n",
       "    differentiable: False\n",
       "    eps: 1e-10\n",
       "    foreach: None\n",
       "    initial_accumulator_value: 0\n",
       "    lr: 0.01\n",
       "    lr_decay: 0\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params(iterable)为待优化参数的iterable或者是定义了参数组的dict\n",
    "# lr(float)为学习率\n",
    "# lr_decay(float,可选)为学习率衰减(默认：0)，\n",
    "# weight_decay(float,可选)为权值衰减，选择一个合适的权值衰减系数非常重要(默认：0)\n",
    "torch.optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params(iterable)为待优化参数的iterable或者是定义了参数组的dict\n",
    "# lr(float)为学习率\n",
    "# alpha(float,可选)为平滑常数(默认：0.99)，\n",
    "# eps(fload,可选)为数值稳定性添加到分母的值（默认：1e-8）\n",
    "# weight_decay(float,可选)为权值衰减，选择一个合适的权值衰减系数非常重要(默认：0)\n",
    "# centered(bool,可选)如果为True，计算中心化的RMSProp，并用它的方差预测值对梯度进行归一化\n",
    "\n",
    "# torch.optim.RMSProp(model.parameters(), lr=0.01, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# params(iterable)为待优化参数的iterable或者是定义了参数组的dict\n",
    "# lr(float)为学习率(默认：0.001)\n",
    "# betas(Tuple[float,float],可选)为用于计算梯度及梯度平方的运行平均值的系数(默认：0.9,0.999)，\n",
    "# eps(fload,可选)为数值稳定性添加到分母的值（默认：1e-8）\n",
    "# weight_decay(float,可选)为权值衰减，选择一个合适的权值衰减系数非常重要(默认：0)\n",
    "\n",
    "torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.999), eps=1e-8, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练测试模块\n",
    "见模版"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二维卷积模板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.datasets import MNIST\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # 原始图片28*28,模型输入要求32*32\n",
    "# data_train = MNIST(r'./data',\n",
    "#                    download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.Resize((32, 32)),\n",
    "#                        transforms.ToTensor()])\n",
    "#                   )\n",
    "# data_test = MNIST(r'./data',\n",
    "#                   train=False,\n",
    "#                   download=True,\n",
    "#                   transform=transforms.Compose([\n",
    "#                       transforms.Resize((32, 32)),\n",
    "#                       transforms.ToTensor()])\n",
    "#                   )\n",
    "# # num_workers指的是进程数,训练集变成了235个256*1*32*32的张量\n",
    "# data_train_loader = DataLoader(data_train, batch_size=256, shuffle=True, num_workers=8)\n",
    "# data_test_loader = DataLoader(data_test, batch_size=1024, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "\n",
    "# class LeNet5(nn.Module):\n",
    "#     def __init__(self, num_classes=10):\n",
    "#         super(LeNet5, self).__init__()\n",
    "\n",
    "#         # 模型每次输入一个batch\n",
    "#         # 输入张量默认内存排列N*C*H*W，输出张量默认内存排列N*K*H1*W1\n",
    "#         # N是批次大小，C是输入通道数，H是高度，W是宽度,K是输出通道数\n",
    "\n",
    "#         # nn.Sequential将一系列层结构进行打包组合成新结构,features是专门用于提取特征\n",
    "#         self.features = nn.Sequential(\n",
    "#             # ConvNd(C,K,(kerner_shape_h,kerner_shape_w,kerner_shape_d),(stride_shape_h,stride_shape_w,stride_shape_d))\n",
    "#             # 如果传入的是元组会在高度和宽度方向上移动，如果传入的是数会在高度或宽度方向上移动\n",
    "#             nn.Conv2d(1, 6, 5, 1), nn.ReLU(),\n",
    "#             # MaxPoolNd(kerner_shape,stride_shape)\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Conv2d(6, 16, 5, 1), nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Flatten()\n",
    "#         )\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             # Linear(in_features,out_features,bias=True)\n",
    "#             nn.Linear(16 * 5 * 5, 120), nn.ReLU(),\n",
    "#             nn.Linear(120, 84), nn.ReLU(),\n",
    "#             nn.Linear(84, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         x = self.classifier(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # 测试输出\n",
    "# if __name__ == '__main__':\n",
    "#     X = torch.randn(1, 1, 32, 32)\n",
    "#     model = LeNet5(num_classes=10)\n",
    "\n",
    "#     for layer in model.features:\n",
    "#         X = layer(X)\n",
    "#         print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "#     for layer in model.classifier:\n",
    "#         X = layer(X)\n",
    "#         print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "#     # 输入一张图片，测试输出结果\n",
    "#     X = torch.randn(1, 1, 32, 32)\n",
    "#     ret = model.forward(X)\n",
    "#     print(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "方法一：数据（imgs，targets），模型，loss函数后面加.cuda来进行gpu训练（训练模型和loss可以不用另外赋值，直接.cuda()，但是数据必须要进行赋值）\n",
    "\n",
    "#模型定义\n",
    "\n",
    "tudui = Tudui()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    tudui.cuda(device=0)\n",
    "    \n",
    "#loss函数\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    loss_fn.cuda(device=0)\n",
    "    \n",
    "#imgs和targets\n",
    "\n",
    "imgs,targets = data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    imgs=imgs.cuda(device=0)\n",
    "    \n",
    "    targets=targets.cuda(device=0)\n",
    "    \n",
    "方法二：在训练模型，数据，loss后.to(device)，提前定义torch.device()(\"cpu\",\"cuda:0\",\"cuda:1\")，\n",
    "\n",
    "#定义训练的设备\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tudui = Tudui()\n",
    "\n",
    "tudui = tudui.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch import optim\n",
    "# from data import data_train_loader\n",
    "# from model import LeNet5\n",
    "# # pip install tensorflow-tensorboard==1.5.1\n",
    "# # pip install tensorboard==1.15.0\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# def train_model(data_train_loader, model, criterion, optimizer, epochs)\n",
    "#     model.train()\n",
    "#     total_loss = 0                             # 记录每个epoch的loss\n",
    "#     train_batch_num = len(data_train_loader)   # 记录每个epoch有多少个batch\n",
    "#     correct = 0                                # 记录每个epoch共有多少个样本被正确分类\n",
    "#     sample_num = 0                             # 记录每个epoch的样本数\n",
    "#     for epoch in range(epochs):\n",
    "#         for batch_index, (inputs, targets) in enumerate(data_train_loader):\n",
    "#             # 正向传播\n",
    "#             inputs, targets = inputs.cuda(device=0), targets.cuda(device=0)\n",
    "#             outputs = model.forward(inputs)\n",
    "#             loss = criterion(outputs,targets) \n",
    "#             # 反向传播\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()   # 累加loss\n",
    "#             prediction = torch.argmax(outputs, 1)\n",
    "#             correct += (prediction == targets).sum().item()\n",
    "#             sample_num += len(prediction)\n",
    "#         loss = total_loss/train_batch_num\n",
    "#         acc = correct / sample_num\n",
    "#         print(epoch, 'loss:%.3f | acc:%.3f%%(%d/%d)'%(loss, 100*acc, correct, sample_num))\n",
    "            \n",
    "# model = LeNet5()\n",
    "# model.cuda(device=0)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion.cuda(device=0)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "# train_model(data_train_loader, model, criterion, optimizer, epochs)\n",
    "\n",
    "# save_info = {'model':model.state_dict(),\n",
    "#              'optimizer':optimizer.state_dict()\n",
    "#             }\n",
    "# # 保存信息\n",
    "# torch.save(save_info,'./model.pth')\n",
    "# # 载入信息\n",
    "# # save_info = torch.load('./model.pth')\n",
    "# # model.load_state_dict(save_info['model'])\n",
    "# # optimizer.load_state_dict(save_info['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from model import LeNet5\n",
    "# from data import data_test_loader\n",
    "\n",
    "# def test_model(data_test_loader, model, criterion)\n",
    "#     model.eval()     # 保证每个参数都固定\n",
    "#     total_loss = 0                             \n",
    "#     test_batch_num = len(data_test_loader)     \n",
    "#     correct = 0                               \n",
    "#     sample_num = 0                           \n",
    "#     with torch.no_grad():       # 不进行梯度变化\n",
    "#         for batch_index, (inputs, targets) in enumerate(data_test_loader):\n",
    "#             # 正向\n",
    "#             inputs, targets = inputs.cuda(device=0), targets.cuda(device=0)\n",
    "#             outputs = model.forward(inputs)\n",
    "#             loss = criterion(outputs,targets) \n",
    "#             total_loss += loss.item()   \n",
    "#             prediction = torch.argmax(outputs, 1)\n",
    "#             correct += (prediction == targets).sum().item()\n",
    "#             sample_num += len(prediction)\n",
    "#         loss = total_loss/test_batch_num\n",
    "#         acc = correct / sample_num\n",
    "#         print('loss:%.3f | acc:%.3f%%(%d/%d)'%(loss, 100*acc, correct, sample_num))\n",
    "            \n",
    "# save_info = torch.load('./model.pth')\n",
    "# model = LeNet5()\n",
    "# model.load_state_dict(save_info['model'])\n",
    "# model.cuda(device=0)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion.cuda(device=0)\n",
    "# test_model(data_test_loader, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Pytorch Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
